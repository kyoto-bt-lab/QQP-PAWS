{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"main.ipynb","provenance":[{"file_id":"1z8M82ek-x7k4Ltrefi7YhgwG-H4rfD1i","timestamp":1577422550034}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"uhciahhhagGE"},"source":["# Mount Google Drive"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"VadPeFrgaeiG","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","%cd '/content/gdrive/My Drive/Colab Notebooks/QQP-PAWS'"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tCNEscwW-jrn","colab_type":"text"},"source":["# Install libraries"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"BCDR8pcZnGlc","colab":{}},"source":["!bash setup.bash\n","!pip install transformers"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eM0x2LvAnMBC","colab_type":"text"},"source":["# QQP→QQP"]},{"cell_type":"code","metadata":{"id":"_-2Je8rhybu8","colab_type":"code","colab":{}},"source":["!python ./transformers/utils/download_glue_data.py \\\n","    --data_dir=data_glue/ \\\n","    --tasks=QQP"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"40EkN4VOnHLY","colab":{}},"source":["!python ./transformers/examples/run_glue.py \\\n","    --data_dir=./data_glue/QQP/ \\\n","    --model_type=bert \\\n","    --model_name_or_path=\"bert-base-uncased\" \\\n","    --do_lower_case \\\n","    --task_name=qqp \\\n","    --do_train \\\n","    --do_eval \\\n","    --output_dir=./result/qqp_qqp \\\n","    --overwrite_output_dir \\\n","    --num_train_epochs=3 \\\n","    --per_gpu_train_batch_size=64 \\\n","    --per_gpu_eval_batch_size=64 \\\n","    --save_steps=5000 \\"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V6ttmqzR3lRk","colab_type":"text"},"source":["# https://arxiv.org/abs/1904.01130"]},{"cell_type":"markdown","metadata":{"id":"3UECp43Zz_KK","colab_type":"text"},"source":["# QQP→PAWSQQP"]},{"cell_type":"code","metadata":{"id":"ZNh-RBmiV1qy","colab_type":"code","colab":{}},"source":["from transformers import DataProcessor, InputExample\n","import os\n","class PawsQqpProcessor(DataProcessor):\n","    \"\"\"Processor for the Paws QQP data set.\"\"\"\n","\n","    def get_example_from_tensor_dict(self, tensor_dict):\n","        \"\"\"See base class.\"\"\"\n","        return InputExample(\n","            tensor_dict[\"idx\"].numpy(),\n","            tensor_dict[\"question1\"].numpy().decode(\"utf-8\"),\n","            tensor_dict[\"question2\"].numpy().decode(\"utf-8\"),\n","            str(tensor_dict[\"label\"].numpy()),\n","        )\n","\n","    def get_train_examples(self, data_dir):\n","        \"\"\"See base class.\"\"\"\n","        return self._create_examples(self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n","\n","    def get_dev_examples(self, data_dir):\n","        \"\"\"See base class.\"\"\"\n","        return self._create_examples(self._read_tsv(os.path.join(data_dir, \"dev_and_test.tsv\")), \"dev\")\n","\n","    def get_labels(self):\n","        \"\"\"See base class.\"\"\"\n","        return [\"0\", \"1\"]\n","\n","    def _create_examples(self, lines, set_type):\n","        \"\"\"Creates examples for the training and dev sets.\"\"\"\n","        examples = []\n","        for (i, line) in enumerate(lines):\n","            if i == 0:\n","                continue\n","            guid = \"%s-%s\" % (set_type, line[0])\n","            try:\n","                text_a = line[1]\n","                text_b = line[2]\n","                label = line[3]\n","            except IndexError:\n","                continue\n","            examples.append(InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n","        return examples"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"H001BAwMXD07","colab_type":"code","colab":{}},"source":["from transformers import glue_convert_examples_to_features as convert_examples_to_features\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n","import logging\n","def load_and_cache_examples(args, task, tokenizer, evaluate=False):\n","    try:\n","        logger.info(\"Load PAWS QQP data\")\n","    except:\n","        logger = logging.getLogger(__name__)\n","    if args.local_rank not in [-1, 0] and not evaluate:\n","        torch.distributed.barrier()  # Make sure only the first process in distributed training process the dataset, and the others will use the cache\n","\n","    processor = PawsQqpProcessor()\n","    output_mode = output_modes[task]\n","    # Load data features from cache or dataset file\n","    cached_features_file = os.path.join(\n","        args.data_dir,\n","        \"cached_{}_{}_{}_{}\".format(\n","            \"dev\" if evaluate else \"train\",\n","            list(filter(None, args.model_name_or_path.split(\"/\"))).pop(),\n","            str(args.max_seq_length),\n","            str(task),\n","        ),\n","    )\n","    if os.path.exists(cached_features_file) and not args.overwrite_cache:\n","        logger.info(\"Loading features from cached file %s\", cached_features_file)\n","        features = torch.load(cached_features_file)\n","    else:\n","        logger.info(\"Creating features from dataset file at %s\", args.data_dir)\n","        label_list = processor.get_labels()\n","        if task in [\"mnli\", \"mnli-mm\"] and args.model_type in [\"roberta\", \"xlmroberta\"]:\n","            # HACK(label indices are swapped in RoBERTa pretrained model)\n","            label_list[1], label_list[2] = label_list[2], label_list[1]\n","        examples = (\n","            processor.get_dev_examples(args.data_dir) if evaluate else processor.get_train_examples(args.data_dir)\n","        )\n","        features = convert_examples_to_features(\n","            examples,\n","            tokenizer,\n","            label_list=label_list,\n","            max_length=args.max_seq_length,\n","            output_mode=output_mode,\n","            pad_on_left=bool(args.model_type in [\"xlnet\"]),  # pad on the left for xlnet\n","            pad_token=tokenizer.convert_tokens_to_ids([tokenizer.pad_token])[0],\n","            pad_token_segment_id=4 if args.model_type in [\"xlnet\"] else 0,\n","        )\n","        if args.local_rank in [-1, 0]:\n","            logger.info(\"Saving features into cached file %s\", cached_features_file)\n","            torch.save(features, cached_features_file)\n","\n","    if args.local_rank == 0 and not evaluate:\n","        torch.distributed.barrier()  # Make sure only the first process in distributed training process the dataset, and the others will use the cache\n","\n","    # Convert to Tensors and build dataset\n","    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n","    all_attention_mask = torch.tensor([f.attention_mask for f in features], dtype=torch.long)\n","    all_token_type_ids = torch.tensor([f.token_type_ids for f in features], dtype=torch.long)\n","    if output_mode == \"classification\":\n","        all_labels = torch.tensor([f.label for f in features], dtype=torch.long)\n","    elif output_mode == \"regression\":\n","        all_labels = torch.tensor([f.label for f in features], dtype=torch.float)\n","\n","    dataset = TensorDataset(all_input_ids, all_attention_mask, all_token_type_ids, all_labels)\n","    return dataset"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3igYwC8NyL2b","colab_type":"code","colab":{}},"source":["from transformers import BertConfig, BertForSequenceClassification, BertTokenizer\n","from transformers import glue_output_modes as output_modes\n","import argparse\n","import torch\n","import sys\n","sys.path.append(\"./transformers/examples\")\n","from run_glue import evaluate\n","\n","config_class, model_class, tokenizer_class = BertConfig, BertForSequenceClassification, BertTokenizer\n","tokenizer = tokenizer_class.from_pretrained(\"./result/qqp_qqp/\", do_lower_case=True)\n","model = model_class.from_pretrained(\"./result/qqp_qqp/\")\n","\n","args = argparse.Namespace(\n","    output_dir=\"./result/qqp_pawsqqp\",\n","    task_name=\"qqp\",\n","    model_type=\"bert\",\n","    data_dir=\"./data_paws/paws_qqp/output\",\n","    model_name_or_path=\"bert-base-uncased\",\n","    overwrite_cache=False,\n","    local_rank=-1,\n","    max_seq_length=128,\n","    per_gpu_eval_batch_size=64,\n","    n_gpu=1,\n","    )\n","args.output_mode = output_modes[\"qqp\"]\n","args.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(args.device)\n","model.eval()\n","with torch.no_grad():\n","    load_and_cache_examples(args, args.task_name, tokenizer, evaluate=True)\n","    result = evaluate(args, model, tokenizer)\n","    print(\"\\n\", result)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"thJF18-w8_tn","colab_type":"text"},"source":["# QQP+PAWSQQP→PAWSQQP"]},{"cell_type":"code","metadata":{"id":"IQeAFB9I9DSw","colab_type":"code","colab":{}},"source":["from run_glue import train\n","\n","# add default parameters\n","args.max_steps = -1\n","args.gradient_accumulation_steps = 1\n","args.learning_rate = 5e-05\n","args.adam_epsilon = 1e-08\n","args.warmup_steps = 0\n","args.weight_decay = 0.0\n","args.max_grad_norm = 1.0\n","args.logging_steps = 500\n","args.fp16 = False\n","args.seed = 42\n","args.evaluate_during_training = False\n","\n","# Fine-tuning\n","args.output_dir = \"./result/pawsqqp_pawsqqp\"\n","args.num_train_epochs = 3\n","args.per_gpu_train_batch_size = 64\n","args.save_steps = 5000\n","model.train()\n","train_dataset = load_and_cache_examples(args, args.task_name, tokenizer, evaluate=False)\n","global_step, tr_loss = train(args, train_dataset, model, tokenizer)\n","print(\"\\n\", \"global_step = %s, average loss = %s\"%(global_step, tr_loss))\n","\n","# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n","# They can then be reloaded using `from_pretrained()`\n","model_to_save = (\n","    model.module if hasattr(model, \"module\") else model\n",")  # Take care of distributed/parallel training\n","model_to_save.save_pretrained(args.output_dir)\n","tokenizer.save_pretrained(args.output_dir)\n","# Good practice: save your training arguments together with the trained model\n","torch.save(args, os.path.join(args.output_dir, \"training_args.bin\"))\n","\n","model.eval()\n","with torch.no_grad():\n","    result = evaluate(args, model, tokenizer)\n","    print(\"\\n\", result)"],"execution_count":0,"outputs":[]}]}